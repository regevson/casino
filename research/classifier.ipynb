{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as dictionary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positives and negatives into dataframe\n",
    "\n",
    "POS_PATH = 'data/pos_clean.txt'\n",
    "NEG_PATH = 'data/neg_clean.txt'\n",
    "\n",
    "with open(POS_PATH, 'r') as file:\n",
    "    pos = file.readlines()\n",
    "\n",
    "with open(NEG_PATH, 'r') as file:\n",
    "    neg = file.readlines()\n",
    "\n",
    "df_pos = pd.DataFrame({'text': pos})\n",
    "df_neg = pd.DataFrame({'text': neg})\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None  # If no match is found\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = pos_tag(tokens)\n",
    "    lemmatized = []\n",
    "    for word, tag in tags:\n",
    "        tag = get_wordnet_pos(tag)\n",
    "        if tag is None:\n",
    "            continue\n",
    "        word = lemmatizer.lemmatize(word, tag)\n",
    "        lemmatized.append(word)\n",
    "    text = \" \".join(lemmatized)\n",
    "    return text\n",
    "\n",
    "# df_pos['text'] = df_pos['text'].apply(lemmatize)\n",
    "# df_neg['text'] = df_neg['text'].apply(lemmatize)\n",
    "df_pos['text'] = df_pos['text'].apply(lambda x: x.lower())\n",
    "df_neg['text'] = df_neg['text'].apply(lambda x: x.lower())\n",
    "\n",
    "df_pos['label'] = 1 # casino\n",
    "df_neg['label'] = 0 # normal\n",
    "\n",
    "df_pos, df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset balanced\n",
    "\n",
    "print(\"num datapoints\", len(df_pos)+len(df_neg))\n",
    "print(\"num positive datapoints\", len(df_pos))\n",
    "print(\"num negative datapoints\", len(df_neg))\n",
    "\n",
    "min_samples = min(len(df_pos), len(df_neg))\n",
    "\n",
    "# undersample\n",
    "df_pos = df_pos.sample(n=min_samples, random_state=SEED)\n",
    "df_neg = df_neg.sample(n=min_samples, random_state=SEED)\n",
    "\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"num datapoints\", len(df_pos)+len(df_neg))\n",
    "print(\"num positive datapoints\", len(df_pos))\n",
    "print(\"num negative datapoints\", len(df_neg))\n",
    "\n",
    "df_pos, df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "X = pd.concat([df_pos[\"text\"], df_neg[\"text\"]])\n",
    "y = pd.concat([df_pos[\"label\"], df_neg[\"label\"]])\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "test_df = pd.concat([X_test_df, y_test_df], axis=1)\n",
    "\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords (only english and german ones)\n",
    "\n",
    "stop_words = stopwords.words('english') + stopwords.words('german')\n",
    "\n",
    "def remove_stopwords_and_split(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "words_series = train_df['text'].apply(remove_stopwords_and_split)\n",
    "print(words_series)\n",
    "all_words = [word for words_list in words_series for word in words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.DataFrame({'word': all_words})\n",
    "word_freq = word_freq_df['word'].value_counts().reset_index()\n",
    "word_freq.columns = ['word', 'freq']\n",
    "\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-valid words\n",
    "print(\"num words before:\", len(word_freq))\n",
    "new_word_freq = word_freq[word_freq['word'].isin(dictionary.words())]\n",
    "print(\"num words after:\", len(new_word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with frequency less than 10\n",
    "print(\"num words before:\", len(new_word_freq))\n",
    "new_word_freq = new_word_freq[new_word_freq['freq'] >= 10]\n",
    "print(\"num words after:\", len(new_word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = new_word_freq['word'].tolist()\n",
    "\n",
    "with open('bag_of_words.txt', 'w') as file:\n",
    "    for word in bag_of_words:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, enabled=True):\n",
    "    features = []\n",
    "    for _, row in tqdm(df.iterrows(), disable=not enabled, total=len(df), desc=\"Processing rows\"):\n",
    "        text = row['text']\n",
    "        website_words = text.split()\n",
    "        website_words = list(dict.fromkeys(website_words))\n",
    "        feature = [1 if w in website_words else 0 for w in bag_of_words]\n",
    "        features.append(feature)\n",
    "    df['features'] = features\n",
    "    return df\n",
    "train_df = create_features(train_df)\n",
    "test_df = create_features(test_df)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_df['features'].tolist())\n",
    "y_train = np.array(train_df['label'].tolist())\n",
    "\n",
    "X_test = np.array(test_df['features'].tolist())\n",
    "y_test = np.array(test_df['label'].tolist())\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "C = 1.0\n",
    "model_svc = SVC(kernel='linear', C=C, probability=True).fit(X_train, y_train)\n",
    "\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "model_for = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "model_for.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "\n",
    "accuracy_svc = model_svc.score(X_test, y_test)\n",
    "accuracy_log = model_log.score(X_test, y_test)\n",
    "accuracy_for= model_for.score(X_test, y_test)\n",
    "print(f\"FOREST: {accuracy_for * 100:.2f}%\")\n",
    "print(f\"LOGISTIC: {accuracy_log * 100:.2f}%\")\n",
    "print(f\"SVM: {accuracy_svc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "dump(model_for, 'weights_random_forest.joblib')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for plot later\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_test)\n",
    "\n",
    "y_pred_linear = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def repaint_scatter(y):\n",
    "    for idx in range(len(y)):\n",
    "        if y[idx] == 1: # incorrect\n",
    "            y[idx] = -1\n",
    "        else: # correct\n",
    "            if y_train[idx] == 1: # casino\n",
    "                y[idx] = 0\n",
    "            else: # not casino\n",
    "                y[idx] = 1\n",
    "    return y\n",
    "    \n",
    "# Titles for the plots\n",
    "title = 'Model Performance'\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# Scatter plot of the reduced data\n",
    "y = repaint_scatter(np.abs(y_test - y_pred_linear))\n",
    "\n",
    "# green: correctly classified and normal website\n",
    "# red: correctly classified and casino website\n",
    "# blue: incorrectly classified\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.brg)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Accept-Language': 'de-DE,de;q=0.9',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "# 'Accept-Language': 'en-US,de;q=0.9',\n",
    "\n",
    "urls = [\"https://casino.draftkings.com/?page=1\", \"https://en.wikipedia.org/wiki/Elon_Musk\", \"https://slottyvegas.com/?page=1\", \"https://en.wikipedia.org/wiki/Casino_Royale_(2006_film)\", \"https://en.wikipedia.org/wiki/Poker\", \"https://www.lotterien.at/de\", \"https://en.wikipedia.org/wiki/Fake_News\", \"https://de.wikipedia.org/wiki/Fake_News\", \"https://en.wikipedia.org/wiki/Slot_machine\", \"https://www.gambling.net/history/\", \"https://en.wikipedia.org/wiki/Las_Vegas\", \"https://en.wikipedia.org/wiki/Pioneer_Club_Las_Vegas\", \"https://localhistories.org/history-of-gambling-how-people-started-gambling/\", \"https://eur.pokerstars.com/\", \"https://www.apa.org/monitor/2023/07/how-gambling-affects-the-brain\", \"https://www.ruetz.at/\", \"https://www.mpreis.at/vielfalt/unsere-marken/baguette\", \"https://corporate.target.com/careers\"]\n",
    "\n",
    "contents = []\n",
    "for url in urls:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    txt = \"\"\n",
    "    if response.ok:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        txt = soup.get_text().replace(\"\\n\", \" \")\n",
    "        contents.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont = \"\"\"\n",
    "# Baguette | MPREIS Order more, save more! Save up to 15 euros in the online store now Pick up a coupon Product or topic Search for products or content Search Login Favorite products PromotionsFoodDrinksDrugstoreInspirationMPREIS All products in promotion Flyer Online promotions MPREIS Specials Topics & novelties NoveltiesFill up stockUse instead of wasteEverything for Törggelen Show all Conscious nutrition Vegan and vegetarian foodGluten-free nutritionLactose-free nutritionFull Tyrol Show all Fruit ApplesBananasBerries Show all Vegetables PotatoesTomatoesCorn, Peppers & Chili Show all Milk & Eggs MilkPlant-based milk substitutesEggs Show all Meat & Sausage MeatSausageSausages Show all Frozen Ice creamPrepared mealsPizza & Baguette Show all Sweet & Salty ChocolateNibblesBiscuits & Biscuits Show all Canned food, Ready Meals & Soups Canned FoodPrepared MealsSoups Show all Bread & Pastries BreadPastriesCakes & Confectionery Show all Basic Food Cereals & MuesliPastaRice Show all To-Go SaladsSandwiches & WrapsDumplings Show all Fish Themes & Novelties Wine AdviceBeer in actionOrder more online Show all Water, Lemonades & energy drinks Mineral waterCola & lemonadesEnergy & sports drinks Show all Juices & syrups SyrupsVegetable juicesFruit juices & smoothies Show all Coffee, Tea & Cocoa CoffeeTeaCocoa Show all Beer Non-alcoholic BeerRadler Show all Wine & Sparkling Wine Sparkling Wine & Champagne Spirits WhiskyWodkaRum Show all Themes & Novelties Drugstore in actionEverything for your four-legged friendsEverything for your personal hygiene Show all Washing & cleaning agents Washing agentsCleaning agents Care Dental & oral hygiene & Oral CareHygieneFacial Care Show all Household Paper productsWaste bagsCandles Show all Pets Cat foodDog foodPet food Show all Baby DiapersBaby careBottles & Pacifiers Show all Flowers HouseplantsFlower seedsPlant accessories Show all Recipes Almond stars Railwayman's leek and cheese soup More about Recipes Stories 8. December open 1+1 day ski pass Axamer Lizum Advent market Bäckerei Therese Mölk More to Stories Regional Regional Tyrol - Currently in the range at MPREIS 20 years of BIO vom BERG Organic variety from South Tyrol More to Regional Recommendations for you Competition Order 4x - win a 500 EURO coupon 500 EURO coupon Recipe 1 dough 5 types of cookies About MPREIS Company & values Jobs & careers Locations & opening hours All about MPREIS Our brands Alpine butcher's shop Bakery Therese Mölk Baguette All our brands Sustainability & values Green mobility Energy Biodiversity More about Sustainability & values Recommendations for you Christmas YOU are the celebration. We take care of the enjoyment.  Brand: Bäckerei Therese Mölk THE WHOLE DAY FOR EVERY TASTE Baguette More than a bistro since 1989. Whether for coffee, breakfast, lunch, a quick snack between meals, cake or simply to buy bread - Baguette not only offers you all kinds of varied offers in over 170 branches in Tyrol, Salzburg, Vorarlberg, Carinthia and Upper Austria, but also the opportunity to take a relaxing break in our feel-good corners. \\xa0 \\xa0 Breakfast assortmentLunch dishesSustainabilityOpening hoursJobs Breakfast assortment From savory to sweet. Start the day with our delicious breakfast options and enjoy the second cup for free! Available daily until 11:00 am.                              Das Kleine 1 oven-fresh pastry* 2 spreads of your choice 1 hot drink (Fairtrade coffee, organic tea or cocoa) Second cup free * except Danish pastries \\xa0 Das\\xa0Süße 1 oven-fresh pastry, cake or strudel\\xa0 1 hot drink (Fairtrade coffee, Organic coffee, organic tea or cocoa) Second cup free of charge \\xa0 The vegetarian Also available vegan 1 vegetarian / vegan snack 1 hot drink (Fairtrade coffee, organic tea or cocoa) Second cup free of charge \\xa0 The savory 1\\xa0piquant filled snack (meat and cheese roll from 09: 00 am) 1 hot drink (Fairtrade coffee, organic tea or cocoa) Second cup free of charge \\xa0 \\xa0 Das\\xa0Große 2\\xa0ofenfrische Kleingebäcke* 3 spreads of your choice 1 hot drink (Fairtrade coffee, organic tea or cocoa) Second cup free of charge * except Danish pastries \\xa0 F\n",
    "# \"\"\"\n",
    "# contents = [cont]\n",
    "# urls = ['tst.com']\n",
    "\n",
    "\n",
    "\n",
    "models = [model_for, model_log, model_svc]\n",
    "model_names = [\"FOREST\", \"LOG\", \"SVM\"]\n",
    "for m, model in enumerate(models):\n",
    "    print(f\"Model: {model_names[m]}\")\n",
    "    for i, txt in enumerate(contents):\n",
    "        txt_clean = txt.lower()\n",
    "        txt_clean = txt_clean.split()\n",
    "        txt_clean = list(set(txt_clean))\n",
    "        txt_clean = ' '.join(txt_clean)\n",
    "        # txt_clean = lemmatize(txt_clean)\n",
    "\n",
    "        df_predict = pd.DataFrame([txt_clean], columns=['text'])\n",
    "\n",
    "        bag_of_words = []\n",
    "        with open('bag_of_words.txt', 'r') as file:\n",
    "            bag_of_words = file.readlines()\n",
    "        bag_of_words = [word.strip() for word in bag_of_words]\n",
    "        df_predict = create_features(df_predict, False)\n",
    "\n",
    "        feature_vec = np.array(df_predict['features'].tolist())\n",
    "\n",
    "        # model = load('weights_random_forest.joblib')\n",
    "        label = model.predict(feature_vec)\n",
    "        prob = model.predict_proba(feature_vec)\n",
    "        print(f\"RESULT: {urls[i]}\", label[0], prob[0][label[0]])\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
