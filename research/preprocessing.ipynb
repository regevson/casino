{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from playwright.async_api import async_playwright\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# scrape website content from url-list\n",
    "\n",
    "URLS_PATH = 'data/blocklist/all_urls_pos.txt'\n",
    "SCRAPE_OUTPUT_PATH = 'data/blocklist/pos_raw.txt'\n",
    "DEAD_URLS_PATH = 'data/blocklist/dead_urls_pos.txt'\n",
    "\n",
    "# store urls with connectivity issues (i.e. dead urls) in own file\n",
    "def write_dead(url):\n",
    "    with open(DEAD_URLS_PATH, \"a\") as out_file:\n",
    "        out_file.write(url+\"\\n\")\n",
    "\n",
    "TIMEOUT = 10 # seconds\n",
    "async def run():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.set_extra_http_headers({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Accept-Language': 'en-US,en;q=0.5' # set preferred language to english\n",
    "        })\n",
    "\n",
    "        with open(URLS_PATH, 'r') as file:\n",
    "            urls = file.readlines()\n",
    "        \n",
    "        for url in tqdm(urls, desc=\"Fetching URLs\"):\n",
    "            url = url.strip() # remove whitespaces and newlines from beginning/end\n",
    "\n",
    "            try:\n",
    "                await page.goto(\"https://\"+url, timeout=TIMEOUT*1000)\n",
    "                content = await page.content()\n",
    "                soup = BeautifulSoup(content, \"html.parser\")\n",
    "                content = soup.get_text(strip=True) # extract visible text only and strip it wrt. spaces\n",
    "            except Exception as e:\n",
    "                write_dead(url) # something went wrong with this url\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # the preprocessing here is only done for readability of the raw data and easier handling\n",
    "            if content: # some valid text was obtained\n",
    "                content = re.sub(r'[^\\w .,;:!?\"\\'-]', ' ', content, flags=re.UNICODE) # only retain valid characters\n",
    "                content = re.sub(r'\\s+', ' ', content) # compress multiple spaces into one\n",
    "            \n",
    "                with open(SCRAPE_OUTPUT_PATH, \"a\") as out_file:\n",
    "                    out_file.write(f\"{content}\\n\")\n",
    "            else: # empty content was obained\n",
    "                write_dead(url)\n",
    "\n",
    "        await context.close()\n",
    "        await browser.close()\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dead urls\n",
    "\n",
    "ALL_URLS_PATH = \"data/blocklist/all_urls_pos.txt\"\n",
    "DEAD_URLS_PATH = \"data/blocklist/dead_urls_pos.txt\"\n",
    "ALIVE_URLS_PATH = \"data/blocklist/alive_urls_pos.txt\"\n",
    "\n",
    "with open(ALL_URLS_PATH, \"r\") as input_file:\n",
    "    all_urls = input_file.readlines()\n",
    "\n",
    "with open(DEAD_URLS_PATH, \"r\") as dead_file:\n",
    "    dead_urls = dead_file.readlines()\n",
    "\n",
    "all_urls_set = set(all_urls) # converted to set to get 'difference' function\n",
    "dead_urls_set = set(dead_urls) # converted to set to get 'difference' function\n",
    "alive_urls = list(all_urls_set.difference(dead_urls_set))\n",
    "\n",
    "with open(ALIVE_URLS_PATH, \"w\") as out_file:\n",
    "    out_file.write(\"\".join(alive_urls))\n",
    "\n",
    "print(\"all urls:\", len(all_urls))\n",
    "print(\"dead urls:\", len(dead_urls))\n",
    "print(\"alive urls:\", len(alive_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSITIVES\n",
    "DATA1_PATH = 'data/blocklist/pos_raw.txt'\n",
    "DATA2_PATH = 'data/gov/pos_raw.txt'\n",
    "DATA_CLEAN_PATH = 'data/pos_clean.txt'\n",
    "\n",
    "# NEGATIVES\n",
    "# DATA1_PATH = 'data/neg_raw.txt'\n",
    "# DATA_CLEAN_PATH = 'data/neg_clean.txt'\n",
    "\n",
    "# minimum #words a website should have\n",
    "# websites with small #words are often ones that only display a js/cookie-warning and should be discarded\n",
    "WORD_THRES = 200\n",
    "\n",
    "with open(DATA1_PATH, 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "with open(DATA2_PATH, 'r') as file:\n",
    "    data = data + file.readlines()\n",
    "\n",
    "print(\"num data before:\", len(data))\n",
    "\n",
    "data = list(set(data)) # remove duplicate website content\n",
    "\n",
    "data_clean = []\n",
    "data_removed = []\n",
    "for d in data:\n",
    "    d = re.sub(r'[^\\w .,;:!?\"\\'-]', ' ', d, flags=re.UNICODE) # discard invalid characters\n",
    "    d = re.sub(r'\\s+', ' ', d) # compress multiple spaces into one\n",
    "    d = d.strip() # remove spaces/newlines in front and end\n",
    "    if len(d.split()) <= WORD_THRES:\n",
    "        data_removed.append(d)\n",
    "        continue\n",
    "    data_clean.append(d)\n",
    "\n",
    "with open(DATA_CLEAN_PATH, 'w') as file:\n",
    "    file.write('\\n'.join(data_clean))\n",
    "\n",
    "print(\"num data after:\", len(data_clean))\n",
    "print(\"data removed due to length:\", len(data_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate what data was removed due to length\n",
    "# here it should be validated that the discarded content is mostly short js/cookie-warnings\n",
    "\n",
    "print(\"data removed due to length (in descending order wrt. words):\")\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "for p in sorted(data_removed, key=lambda x: len(x.split()), reverse=True):\n",
    "    if p != \"\\n\":\n",
    "        print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
